# This coinfiguration is a test that training on DSM2 can be resumed.
# Its origin comes from a bug in TF 2.7.4 where saving and loading a model 
# did not preserve weights. I've kept it as a correctness check.

steps:
- name: dsm2_base
  input_prefix: dsm2_base
  input_mask_regex: None     # regex to filter out (not implemented)
  output_prefix: dsm2_base_gru2
  save_model_fname: dsm2_base_gru2.h5
  load_model_fname: None
  pool_size: 11
  target_fold_length: 180d  # "Cases" will be subdivided into "folds" so that folds fall close to this length. 
  pool_aggregation: True    # If True, collapses the number of folds to pool_size Helpful for big data batches with many cases, here used for 100yr DSM2
  init_train_rate: 0.008
  init_epochs: 10
  main_train_rate: 0.001
  main_epochs: 10           # Kind of a token effort here to do a second round.
- name: dsm2_restart
  input_prefix: dsm2_base
  input_mask_regex: None     # regex to filter out (not implemented)
  output_prefix: dsm2.restart_base_gru2
  save_model_fname: dsm2.restart_base_gru2.h5
  load_model_fname: dsm2_base_gru2.h5
  #load_model_fname: None
  pool_size: 10
  pool_aggregation: True
  target_fold_length: 180d
  init_train_rate: 0.001  # Only one round at low rate for transfer learning
  init_epochs: 10
  main_train_rate: None   # Did not bother with a second round since the training rate is the same as where we left off