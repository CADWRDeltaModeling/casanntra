# # ===========================
# # TRANSFER LEARNING CONFIGURATION FILE
# # ===========================
# # This file defines the training sequence for staged transfer learning.
# # Each step builds upon prior models, allowing for progressive learning.
# # The user can selectively re-execute steps as needed.
# # ===========================

model_builder_config:
  # Defines which ModelBuilder subclass is used.
  builder_name: MultiStageModelBuilder  # Uses a staged learning model
  args:
    input_names:  # Input features
      - northern_flow
      - exports
      - sjr_flow
      - cu_flow
      - sf_tidal_energy
      - sf_tidal_filter
      - dcc
      - smscg
    output_names:  # Output variables with scaling factors
      x2: 100.0    # Normalized based on known physical ranges
      pct: 12000.0
      mal: 12000.0  # 2
      god: 12000.0
      vol: 12000.0
      bdl: 12000.0  # 5
      nsl2: 12000.0
      cse: 12000.0
      emm2: 3000.0
      tms: 3000.0    # 9
      anh: 3000.0
      jer: 2500.0
      gzl: 12000.0   #12
      sal: 12000.0
      frk: 2500.0
      bac: 1500.0
      rsl: 1500.0
      oh4: 1500.0
      trp: 1500.0    #18

    ndays: 90  # Defines the time window for GRU model (90 days)

# # ===========================
# # TRAINING STEPS
# # ===========================
# # Each step corresponds to a training phase. Models are trained in stages.
# # ===========================

steps:
- name: dsm2_base # Step 1: Train DSM2 (1D Model) on its original dataset
  input_prefix: dsm2_base # Points to DSM2 training data
  input_mask_regex: [dsm2_base_100.\.csv] # Filters input files
  output_prefix: dsm2_base_gru2_test # Prefix for saved results
  save_model_fname: dsm2_base_gru2_test # Final model filename
  load_model_fname: null # First modelâ€”no prior model to load
  pool_size: 12 # Number of parallel training threads
  pool_aggregation: true # Use pool aggregation for large case counts
  target_fold_length: 180d # Cross-validation time fold length
  init_train_rate: 0.008 # Initial learning rate for warm-up
  init_epochs: 10 # Short warm-up phase
  main_train_rate: 0.001 # Standard learning rate for main training phase
  main_epochs: 100 # Number of epochs for main training phase was 100
  builder_args:
    transfer_type: direct
    feature_layers:
      - {type: GRU, units: 32, name: lay1}
      - {type: GRU, units: 16, name: lay2}
    ann_output_name: dsm2_base_test

# --------------------------------------------------------------
- name: dsm2.schism # Step 2: Transfer DSM2 Knowledge to SCHISM (3D Model)
  input_prefix: schism_base # Points to SCHISM dataset (aligned with DSM2)
  input_mask_regex: null # No need for filtering
  output_prefix: dsm2.schism_base_gru2_test # New model output filename
  save_model_fname: dsm2.schism_base_gru2_test # File to save the transfer-learned model
  load_model_fname: dsm2_base_gru2_test # Start from DSM2 model
  pool_size: 12
  pool_aggregation: false
  target_fold_length: 180d
  init_train_rate: 0.003 # Medium learning rate, helpful but tune down if you see catastrophic forgetting (see docs)
  init_epochs: 10 # Transfer learning often requires fewer epochs was 10 then 35
  main_train_rate: 0.001 # Only a single training phase
  main_epochs: 35 # No separate main training phase was 25
  builder_args:
    transfer_type: direct # Standard transfer learning
    feature_layers:
      - {type: GRU, units: 32, name: lay1, trainable: true} 
      - {type: GRU, units: 16, name: lay2, trainable: true} 
    save_modified_orig_model_fname: null
    ann_output_name: schism_base_test

# --------------------------------------------------------------
- name: base.suisun # Step 3: Transfer SCHISM (Base Scenario) to SCHISM (Suisun Scenario)
  input_prefix: schism_suisun # Dataset for the Suisun scenario
  input_mask_regex: null
  output_prefix: schism_base.suisun_gru2_test # New model name
  save_model_fname: schism_base.suisun_gru2_test  # Final model filename
  load_model_fname: dsm2.schism_base_gru2_test # Start from SCHISM (Base)
  pool_size: 12
  pool_aggregation: false
  target_fold_length: 180d
  init_train_rate: 0.001 # Even lower learning rate for fine-tuning
  init_epochs: 10
  main_train_rate: 0.0004 # Main learning rate for adaptation
  main_epochs: 36 # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: schism_base
    source_input_mask_regex: [schism_base_2021.csv]
    transfer_type: contrastive
    contrast_weight: 0.5
    feature_layers:
      - {type: GRU, units: 32, name: lay1, trainable: false}
      - {type: GRU, units: 16, name: lay2, trainable: false}
    save_modified_orig_model_fname: schism_base-suisun_gru2_test