# ===========================
# TRANSFER LEARNING CONFIGURATION FILE
# ===========================
# This file defines the training sequence for staged transfer learning.
# Each step builds upon prior models, allowing for progressive learning.
# The user can selectively re‑execute steps as needed.
# ===========================

output_dir: "./output"      # Directory for saving output files

model_builder_config:
# Defines which ModelBuilder subclass is used.
  builder_name: MultiStageModelBuilder # Uses a staged learning model
  args:
    input_names: # Features (input variables)
      - ndo
      - sf_tidal_energy
      - sf_tidal_filter
      - smscg
    output_names: # Output variables with their scaling factors
      x2:  100.0   # Normalized based on known physical ranges
      mrz: 30000.0
      pct: 20000.0
      mal: 15000.0 # 2
      bdl: 14000.0 # 5
      cse: 12000.0
      emm2: 4000.0
      anh: 6000.0
      jer: 3000.0
      gzl: 24000.0 #12
    ndays: 90

# ===========================
# TRAINING STEPS
# ===========================

steps:
# --------------------------------------------------------------
- name: x2_dsm2_base # Step 1: Train DSM2 (1D Model) on its original dataset
  input_prefix: dsm2_base # Points to DSM2 training data
  input_mask_regex: [dsm2_base_historical\.csv, dsm2_base_calsim\.csv] # Filters input files
  output_prefix: "{output_dir}/x2_dsm2_base_gru2" # Prefix for saved results
  save_model_fname: "{output_dir}/x2_dsm2_base_gru2" # Final model filename
  load_model_fname: None # First model—no prior model to load
  pool_size: 12 # Number of parallel training threads
  pool_aggregation: True # Use pool aggregation for large case counts
  target_fold_length: 180d # Cross-validation time fold length
  init_train_rate: 0.008 # Initial learning rate for warm-up
  init_epochs: 10 # Short warm-up phase
  main_train_rate: 0.001 # Standard learning rate for main training phase
  main_epochs: 100 # Number of epochs for main training phase was 100
  builder_args:
    transfer_type: None # No transfer learning in the base DSM2 training
    ann_output_name: x2_dsm2_base # Name of output (applies if there is one output head, otherwise use ann_output_name_map)
    feature_layers:          
      - {type: LSTM, units: 32, name: lay1}
      - {type: LSTM, units: 16, name: lay2}

# --------------------------------------------------------------
- name: x2_dsm2.schism # Step 2: Transfer learning from DSM2 to SCHISM
  input_prefix: schism_base # Points to SCHISM training data
  input_mask_regex: None # No need for filtering
  output_prefix: "{output_dir}/x2_dsm2.schism_base_gru2" # New model output filename
  save_model_fname: "{output_dir}/x2_dsm2.schism_base_gru2" # Final model filename
  load_model_fname: "{output_dir}/x2_dsm2_base_gru2" # Start from DSM2 model
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.003 # Medium learning rate, helpful but tune down if you see catostrophic forgetting (see docs)
  init_epochs: 10  # Transfer learning often requires fewer epochs was 10 then 35
  main_train_rate: 0.001 # Only a single training phase
  main_epochs: 35  
  builder_args:
    transfer_type: direct # Standard transfer learning
    save_modified_orig_model_fname: None # No need modify the DSM2 model
    # ann_output_name: x2_schsim_base
    feature_layers:             
      - {type: LSTM, units: 32, name: lay1, trainable: true}
      - {type: LSTM, units: 16, name: lay2, trainable: true}

# --------------------------------------------------------------
- name: x2_base.suisun # Step 3: Transfer SCHISM (Base Scenario) to SCHISM (Suisun Scenario)
  input_prefix: schism_suisun # Dataset for the Suisun scenario
  input_mask_regex: None
  output_prefix: "{output_dir}/x2_schism_base.suisun_gru2" # New model name
  save_model_fname: "{output_dir}/x2_schism_base.suisun_gru2" # Final model filename
  load_model_fname: "{output_dir}/x2_dsm2.schism_base_gru2" # Start from SCHISM (Base)
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.001 # Even lower learning rate for fine-tuning
  init_epochs: 12 # Short fine-tuning phase
  main_train_rate: 0.0004 # Main learning rate for adaptation
  main_epochs: 36 # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: schism_base
    source_input_mask_regex: [schism_base_2021.csv ]
    transfer_type: contrastive # Use contrastive learning to retain differences
    contrast_weight: 0.5
    save_modified_orig_model_fname: x2_schism_base-suisun_gru2
    feature_layers:               
      - {type: LSTM, units: 32, name: lay1, trainable: true}
      - {type: LSTM, units: 16, name: lay2, trainable: true}